{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUQgXjNK-rK8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "from io import StringIO\n",
        "\n",
        "def normalize_text_columns(df):\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        df[col] = df[col].apply(\n",
        "            lambda x: unicodedata.normalize(\"NFKD\", x).encode(\"ascii\", \"ignore\").decode(\"ascii\") if isinstance(x, str) else x\n",
        "        )\n",
        "    return df\n",
        "\n",
        "# Read and fix decoding issues using open() + StringIO\n",
        "def load_and_clean_csv(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        content = f.read()\n",
        "    df = pd.read_csv(StringIO(content))\n",
        "    df = normalize_text_columns(df)\n",
        "    return df\n",
        "\n",
        "# Function to check for encoding issues\n",
        "def find_bad_encoding_line(file_path, encoding='utf-8'):\n",
        "    bad_lines = []\n",
        "    with open(file_path, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            try:\n",
        "                line.decode(encoding)\n",
        "            except UnicodeDecodeError as e:\n",
        "                bad_lines.append((i + 1, str(e), line))\n",
        "\n",
        "    if bad_lines:\n",
        "        print(f\" Found {len(bad_lines)} bad line(s) in '{file_path}':\\n\")\n",
        "        for line_num, error_msg, raw_bytes in bad_lines:\n",
        "            print(f\" Line {line_num}: {error_msg}\")\n",
        "            print(f\" Bytes: {raw_bytes}\\n\")\n",
        "    else:\n",
        "        print(f\" All lines decoded successfully in '{file_path}'.\")\n",
        "\n",
        "news_file = \"/content/hul_only_et.csv\"\n",
        "trading_file = \"/content/HUL_2019_to_2024.csv\"\n",
        "\n",
        "# # Check for bad encoding in the cleaned files\n",
        "print(\"\\nChecking bad encoding in cleaned news file...\")\n",
        "find_bad_encoding_line(news_file)\n",
        "\n",
        "print(\"\\nChecking bad encoding in cleaned trading file...\")\n",
        "find_bad_encoding_line(trading_file)\n",
        "\n",
        "# Load and clean the files\n",
        "# news_df = load_and_clean_csv(news_file)\n",
        "# trading_df = load_and_clean_csv(trading_file)\n",
        "\n",
        "# # Now that the data is cleaned, let's save it and check if the cleaned data has any encoding issues\n",
        "# cleaned_news_file = \"/content/Cleaned_TCS_all_news.csv\"\n",
        "# cleaned_trading_file = \"/content/Cleaned_TCS_2019_to_2024.csv\"\n",
        "\n",
        "# # Save the cleaned data to new files\n",
        "# news_df.to_csv(cleaned_news_file, index=False)\n",
        "# trading_df.to_csv(cleaned_trading_file, index=False)\n",
        "\n",
        "# # Check for bad encoding in the cleaned files\n",
        "# print(\"\\nChecking bad encoding in cleaned news file...\")\n",
        "# find_bad_encoding_line(cleaned_news_file)\n",
        "\n",
        "# print(\"\\nChecking bad encoding in cleaned trading file...\")\n",
        "# find_bad_encoding_line(cleaned_trading_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "from google.colab import drive, files\n",
        "\n",
        "#  Step 2: Define File Paths\n",
        "news_file = \"/content/hul_only_et.csv\"\n",
        "trading_file = \"/content/HUL_2019_to_2024.csv\"\n",
        "\n",
        "#  Step 3: Sentiment Analyzer Class\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self, model_name=\"finbert\"):\n",
        "        self.model_name = model_name\n",
        "        self.model_configs = {\n",
        "            'finbert': {'model': 'ProsusAI/finbert', 'labels': ['positive', 'negative', 'neutral']}\n",
        "        }\n",
        "        self.config = self.model_configs[model_name.lower()]\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config['model'])\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.config['model'])\n",
        "        self.nlp = pipeline(\"sentiment-analysis\", model=self.model, tokenizer=self.tokenizer)\n",
        "\n",
        "    def analyze_batch(self, texts):\n",
        "        with torch.no_grad():\n",
        "            results = self.nlp(texts, truncation=True, max_length=512)\n",
        "        return [self._convert_to_score(res) for res in results]\n",
        "\n",
        "    def _convert_to_score(self, result):\n",
        "        sentiment = result['label'].lower()\n",
        "        score = -float(result['score']) if sentiment == 'negative' else (0 if sentiment == 'neutral' else float(result['score']))\n",
        "        return score\n",
        "\n",
        "#  Step 4: Process News & Trading Data\n",
        "def process_sentiment(news_file, trading_file):\n",
        "    # Load CSVs\n",
        "    news_df = pd.read_csv(news_file)\n",
        "    trading_df = pd.read_csv(trading_file)\n",
        "\n",
        "    print(\"ðŸ“… Sample news dates:\")\n",
        "    print(news_df['published_date'].dropna().head(10).to_list())\n",
        "\n",
        "    print(\"\\nðŸ“… Sample trading dates:\")\n",
        "    print(trading_df['Date'].dropna().head(10).to_list())\n",
        "\n",
        "\n",
        "    news_df['published_date'] = pd.to_datetime(news_df['published_date'], errors='coerce').dt.normalize()\n",
        "    trading_df['Date']       = pd.to_datetime(trading_df['Date'], format=\"%d-%b-%y\", errors='coerce').dt.normalize()\n",
        "\n",
        "\n",
        "# 2) THEN print samples\n",
        "    print(\"ðŸ“… Sample news dates (postâ€‘parse):\")\n",
        "    print(news_df['published_date'].head(10).to_list())\n",
        "    print(news_df['published_date'].dtype)\n",
        "\n",
        "    print(\"\\nðŸ“… Sample trading dates (postâ€‘parse):\")\n",
        "    print(trading_df['Date'].head(10).to_list())\n",
        "    print(trading_df['Date'].dtype)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize Sentiment Analyzer\n",
        "    analyzer = SentimentAnalyzer()\n",
        "\n",
        "    # Combine title and summary for sentiment analysis\n",
        "    news_df['combined_text'] = news_df['title'].fillna('') + \" \" + news_df['summary'].fillna('')\n",
        "\n",
        "    # Apply sentiment analysis\n",
        "    news_df['sentiment_score'] = analyzer.analyze_batch(news_df['combined_text'].tolist())\n",
        "\n",
        "    #  Step 5: Aggregate Sentiment Per Day\n",
        "    daily_sentiment = news_df.groupby('published_date')['sentiment_score'].sum().reset_index()\n",
        "    daily_sentiment.columns = ['news_date', 'net_sentiment_score']\n",
        "\n",
        "    print(\"ðŸ“° daily_sentiment:\")\n",
        "    print(daily_sentiment.head())\n",
        "    print(daily_sentiment.dtypes)\n",
        "\n",
        "    #  Step 6: Map Sentiment to the Next Available Trading Day\n",
        "    sentiment_map = {}\n",
        "    for date, score in zip(daily_sentiment['news_date'], daily_sentiment['net_sentiment_score']):\n",
        "        next_trading_day = trading_df[trading_df['Date'] >= date]['Date'].min()\n",
        "        if pd.notna(next_trading_day):\n",
        "            sentiment_map[next_trading_day] = sentiment_map.get(next_trading_day, 0) + score\n",
        "\n",
        "    # Convert mapping to DataFrame\n",
        "    sentiment_df = pd.DataFrame(list(sentiment_map.items()), columns=['Date', 'net_sentiment_score'])\n",
        "\n",
        "    #  Step 7: Normalize Sentiment with Volatility Adjustment\n",
        "    if not sentiment_df.empty:\n",
        "        mean_sentiment = sentiment_df['net_sentiment_score'].mean()\n",
        "        std_sentiment = sentiment_df['net_sentiment_score'].std(ddof=0)\n",
        "\n",
        "        # Apply tanh normalization\n",
        "        sentiment_df['normalized_sentiment'] = np.tanh(sentiment_df['net_sentiment_score'])\n",
        "\n",
        "        # Apply volatility adjustment\n",
        "        sentiment_df['normalized_sentiment'] *= (1 - (std_sentiment / (std_sentiment + 1)))\n",
        "\n",
        "        # Keep only the final sentiment column\n",
        "        sentiment_df = sentiment_df[['Date', 'normalized_sentiment']].rename(columns={'normalized_sentiment': 'net_sentiment_score'})\n",
        "\n",
        "    # Merge with trading data\n",
        "    trading_sentiment_df = trading_df.merge(sentiment_df, on='Date', how='left').fillna(0)\n",
        "\n",
        "    #  Step 8: Save the Final Dataset\n",
        "    trading_sentiment_df.to_csv(\"trading_data_with_net_sentiment.csv\", index=False)\n",
        "    news_df[['published_date', 'combined_text', 'sentiment_score']].to_csv(\"news_with_sentiment.csv\", index=False)\n",
        "\n",
        "    print(\" Sentiment analysis and mapping completed!\")\n",
        "\n",
        "    return trading_sentiment_df\n",
        "\n",
        "#  Step 8: Main Guard\n",
        "if __name__ == \"__main__\":\n",
        "    final_trading_df = process_sentiment(news_file, trading_file)\n",
        "\n",
        "    #  Step 9: Download Output Files\n",
        "    files.download(\"trading_data_with_net_sentiment.csv\")\n",
        "    files.download(\"news_with_sentiment.csv\")\n"
      ],
      "metadata": {
        "id": "2FzXm3BNAKI6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}